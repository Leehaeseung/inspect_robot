from __future__ import annotations
import cv2
import torch
import numpy as np
from typing import TYPE_CHECKING

import omni.isaac.lab.utils.math as math_utils
from omni.isaac.lab.assets import Articulation, RigidObject
from omni.isaac.lab.managers import SceneEntityCfg
from omni.isaac.lab.managers.manager_base import ManagerTermBase
from omni.isaac.lab.managers.manager_term_cfg import ObservationTermCfg
from omni.isaac.lab.sensors import Camera, Imu, RayCaster, RayCasterCamera, TiledCamera

if TYPE_CHECKING:
    from omni.isaac.lab.envs import ManagerBasedEnv, ManagerBasedRLEnv
import os
from collections import deque

# from collections import deque
# import numpy as np
# import cv2
# import os
# import torch

# # 64Í∞ú ÌôòÍ≤Ω Í∞ÅÍ∞Å ÏµúÎåÄ 28ÌîÑÎ†àÏûÑÏùÑ Ï†ÄÏû•
# frame_queues = [deque(maxlen=28) for _ in range(64)]

# def image_contour_debug(
#     env: ManagerBasedEnv,
#     sensor_cfg: SceneEntityCfg = SceneEntityCfg("tiled_camera"),
#     data_type: str = "rgb",
#     convert_perspective_to_orthogonal: bool = False,
#     normalize: bool = True
# ) -> torch.Tensor:
#     # 1) ÏÑºÏÑú Îç∞Ïù¥ÌÑ∞ (640√ó320) ÌöçÎìù
#     sensor: TiledCamera | Camera | RayCasterCamera = env.scene.sensors[sensor_cfg.name]
#     images = sensor.data.output[data_type]  

#     # 2) ÏõêÍ∑º ÍπäÏù¥ -> ÏßÅÍµêÌôî (ÏòµÏÖò)
#     if (data_type == "distance_to_camera") and convert_perspective_to_orthogonal:
#         images = math_utils.orthogonalize_perspective_depth(images, sensor.data.intrinsic_matrices)

#     # 3) Ï†ïÍ∑úÌôî (RGB) or depth ÌõÑÏ≤òÎ¶¨
#     if normalize and data_type == "rgb":
#         images = images.float() / 255.0
#         mean_tensor = torch.mean(images, dim=(1, 2), keepdim=True)
#         images -= mean_tensor
#     elif "distance_to" in data_type or "depth" in data_type:
#         images[images == float("inf")] = 0

#     # 4) ÎßåÏïΩ RGB Îç∞Ïù¥ÌÑ∞ÎùºÎ©¥ Canny Ïó£ÏßÄ Ï≤òÎ¶¨
#     if data_type == "rgb":
#         # (64, 320, 640, 3) in [-1,1] -> [0,255]
#         img_np = images.cpu().numpy()
#         img_np = ((img_np + 1) * 127.5).astype(np.uint8)  # shape = (64, 320, 640, 3)

#         # Í∑∏Î†àÏù¥Ïä§ÏºÄÏùº Î≥ÄÌôò
#         img_gray = np.zeros((img_np.shape[0], img_np.shape[1], img_np.shape[2]), dtype=np.uint8)
#         for i in range(img_np.shape[0]):
#             img_gray[i] = cv2.cvtColor(img_np[i], cv2.COLOR_RGB2GRAY)

#         # CLAHE
#         enhanced_images = np.zeros_like(img_gray)
#         clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))
#         for i in range(img_gray.shape[0]):
#             enhanced_images[i] = clahe.apply(img_gray[i])

#         # Canny (640√ó320 Ìï¥ÏÉÅÎèÑ)
#         edge_images = np.zeros_like(enhanced_images)
#         for i in range(enhanced_images.shape[0]):
#             edge_images[i] = cv2.Canny(enhanced_images[i], 25, 35)

#         # 5) ÌÅêÏóê ÌîÑÎ†àÏûÑ Ï∂îÍ∞Ä (deque ÏµúÎåÄ 28)
#         for i in range(64):
#             frame_queues[i].append(edge_images[i])  # (320,640)

#         # 6) 5Í∞ú ÌîÑÎ†àÏûÑ ÏÑ†ÌÉù ([0,7,14,21,27])
#         selected_indices = [0, 7, 14, 21, 27]
#         H, W = edge_images.shape[1], edge_images.shape[2]  # (320,640)
#         stacked_frames = np.zeros((64, 5, H, W), dtype=np.uint8)
#         for i in range(64):
#             frames_list = list(frame_queues[i])
#             for j, idx in enumerate(selected_indices):
#                 if len(frames_list) > idx:
#                     stacked_frames[i, j] = frames_list[idx]
#                 else:
#                     stacked_frames[i, j] = frames_list[-1]

#         # 7) ÎîîÎ≤ÑÍ∑∏(ÌôòÍ≤Ω0 ÏµúÏã† ÌîÑÎ†àÏûÑ)Î•º ASCIIÎ°ú ÌëúÏãú (80√ó40 ÌÅ¨Í∏∞Î°ú ÏãúÍ∞ÅÌôî)
#         os.system('clear' if os.name == 'posix' else 'cls') 
#         print("\nüñ•Ô∏è Contour Detection (ASCII View)\n" + "="*40)

#         ascii_chars = ['.', '#']  
#         # ÌôòÍ≤Ω0Ïùò 5Í∞ú ÌîÑÎ†àÏûÑ Ï§ë ÎßàÏßÄÎßâ(27Î≤àÏß∏), shape=(320,640)
#         debug_frame = stacked_frames[0, -1]
#         # ÎîîÎ≤ÑÍ∑∏ Ïö©ÎèÑÎ°ú (80,40)Îßå ÏîÄ
#         debug_resized = cv2.resize(debug_frame, (80, 40))
#         ascii_img = '\n'.join(
#             ''.join(ascii_chars[1] if pixel > 0 else ascii_chars[0] for pixel in row)
#             for row in debug_resized
#         )
#         print(ascii_img)

#         # 8) ÏµúÏ¢Ö Ï∂úÎ†•: (80√ó80) ÌÅ¨Í∏∞Î°ú Î¶¨ÏÇ¨Ïù¥Ï¶à -> (64, 5, 80,80)
#         #    Ïù¥Ïñ¥ÏÑú Flatten -> (64, 5*80*80)
#         TARGET_SIZE = (80, 80)
#         resized_frames = np.zeros((64, 5, TARGET_SIZE[1], TARGET_SIZE[0]), dtype=np.uint8)
#         for i in range(64):
#             for j in range(5):
#                 resized_frames[i, j] = cv2.resize(stacked_frames[i, j], TARGET_SIZE)

#         contour_tensor = torch.from_numpy(resized_frames).float() / 255.0
#         contour_tensor = contour_tensor.to(images.device)
#         reshaped_tensor = contour_tensor.view(64, -1)

#         return reshaped_tensor

#     # Í∑∏ Ïô∏ ÌÉÄÏûÖÏùÄ Í∑∏ÎÉ• Î∞òÌôò
#     return images

# 64Í∞ú ÌôòÍ≤ΩÎßàÎã§ ÏµúÍ∑º 28ÌîÑÎ†àÏûÑÏùÑ Ï†ÄÏû•
frame_queues = [deque(maxlen=28) for _ in range(64)]

def image_line_debug_latest(
    env: ManagerBasedEnv,
    sensor_cfg: SceneEntityCfg = SceneEntityCfg("tiled_camera"),
    data_type: str = "rgb",
    normalize: bool = True,
    max_edges: int = 100,
    print_debug: bool = True  # üîπ Ï∂îÍ∞Ä: ÎîîÎ≤ÑÍ∑∏ Ï∂úÎ†• Ï†úÏñ¥
) -> torch.Tensor:
    """
    1) 64Í∞ú ÌôòÍ≤ΩÏóêÏÑú ÏµúÍ∑º 28Í∞ú ÌîÑÎ†àÏûÑÏùÑ Ï†ÄÏû•
    2) [0, 7, 14, 21, 27] Î≤àÏß∏ 5Í∞ú ÌîÑÎ†àÏûÑÏùÑ ÏÇ¨Ïö©Ìï¥ ÏÑ†Î∂Ñ Í≤ÄÏ∂ú
    3) ÌôòÍ≤Ω0Ïùò Ï≤´ Î≤àÏß∏ ÌîÑÎ†àÏûÑ(0Î≤àÏß∏)Ïóê ÎåÄÌïú ASCII ÎîîÎ≤ÑÍπÖ ÏàòÌñâ (Ï°∞Í±¥: print_debug=True)
    4) Í∞Å ÌôòÍ≤ΩÎ≥Ñ (5ÌîÑÎ†àÏûÑ x max_edges x 4) FlattenÌïòÏó¨ (64, 5 * max_edges * 4) ÌòïÌÉúÎ°ú Î∞òÌôò
    """

    # 1) ÏÑºÏÑúÎ°úÎ∂ÄÌÑ∞ 64ÌîÑÎ†àÏûÑ(Í∞Å env) ÌöçÎìù (320√ó320)
    sensor: TiledCamera | Camera | RayCasterCamera = env.scene.sensors[sensor_cfg.name]
    images = sensor.data.output[data_type]  # (64, 320, 320, 3)

    # RGB Ï†ïÍ∑úÌôî
    if normalize and data_type == "rgb":
        images = images.float() / 255.0
        mean_tensor = torch.mean(images, dim=(1, 2), keepdim=True)
        images -= mean_tensor

    # [-1,1] Î≤îÏúÑÎ•º [0,255]Î°ú Î≥ÄÌôò
    img_np = images.cpu().numpy()
    img_np = ((img_np + 1) * 127.5).astype(np.uint8)  # (64, 320, 320, 3)

    H, W = img_np.shape[1], img_np.shape[2]
    gray_imgs = np.zeros((64, H, W), dtype=np.uint8)
    for i in range(64):
        gray_imgs[i] = cv2.cvtColor(img_np[i], cv2.COLOR_RGB2GRAY)

    edge_imgs = np.zeros_like(gray_imgs)
    for i in range(64):
        # Ïòà: (5, 30)Î°ú ÏÑ§Ï†ï
        edge_imgs[i] = cv2.Canny(gray_imgs[i], 5, 30)

    # üîπ ÌÅêÏóê ÌîÑÎ†àÏûÑ Ï†ÄÏû•
    for i in range(64):
        frame_queues[i].append(edge_imgs[i])

    # üîπ 5Í∞ú ÌîÑÎ†àÏûÑ ÏÑ†ÌÉù ([0,7,14,21,27])
    selected_indices = [0, 7, 14, 21, 27]
    stacked_frames = np.zeros((64, 5, H, W), dtype=np.uint8)
    for i in range(64):
        frames_list = list(frame_queues[i])
        for j, idx in enumerate(selected_indices):
            if len(frames_list) > idx:
                stacked_frames[i, j] = frames_list[idx]
            else:
                stacked_frames[i, j] = frames_list[-1]

    # üîπ HoughLinesP
    rho = 1
    theta = np.pi / 30
    threshold = 10
    min_line_length = 30
    max_line_gap = 10

    line_features_all = np.zeros((64, 5, max_edges * 4), dtype=np.float32)
    detected_counts = np.zeros((64, 5), dtype=int)

    for env_idx in range(64):
        for frame_idx in range(5):
            frame = stacked_frames[env_idx, frame_idx]
            lines = cv2.HoughLinesP(
                frame, rho, theta, threshold,
                minLineLength=min_line_length, maxLineGap=max_line_gap
            )

            frame_features = []
            if lines is not None:
                lines = lines.reshape(-1, 4)
                detected_counts[env_idx, frame_idx] = lines.shape[0]

                for (x1, y1, x2, y2) in lines:
                    cx = (x1 + x2) / 2.0
                    cy = (y1 + y2) / 2.0
                    length = np.sqrt((x2 - x1)**2 + (y2 - y1)**2)
                    angle = np.arctan2((y2 - y1), (x2 - x1))
                    frame_features.append([cx, cy, angle, length])

                frame_features = sorted(frame_features, key=lambda f: f[3], reverse=True)
                frame_features = frame_features[:max_edges]

            if len(frame_features) < max_edges:
                frame_features += [[0.0, 0.0, 0.0, 0.0]] * (max_edges - len(frame_features))

            line_features_all[env_idx, frame_idx] = np.array(frame_features, dtype=np.float32).flatten()

    # üîπ ÌÑ∞ÎØ∏ÎÑê Ï∂úÎ†• (Ï°∞Í±¥: print_debug=True)
    if print_debug:
        os.system('clear' if os.name == 'posix' else 'cls')
        print("\nüñ•Ô∏è RealTime Edge Debug (ASCII) - First Frame\n" + "="*40)
        print(f"[Env=0] Detected lines = {detected_counts[0, 0]}")

    # ÎîîÎ≤ÑÍ∑∏Ïö© Ïù¥ÎØ∏ÏßÄ (ÌôòÍ≤Ω0, Ï≤´ Î≤àÏß∏ ÌîÑÎ†àÏûÑ)
    debug_img = stacked_frames[0, 0].copy()  # (320,320)
    color_debug = np.stack([debug_img]*3, axis=-1)  # (320,320,3)

    # ÎùºÏù∏ Ï†ïÎ≥¥
    lines_0 = line_features_all[0, 0].reshape(max_edges, 4)
    for (cx, cy, angle, length) in lines_0:
        if length > 0:
            half = length / 2
            dx = np.cos(angle) * half
            dy = np.sin(angle) * half
            x1, y1 = int(cx - dx), int(cy - dy)
            x2, y2 = int(cx + dx), int(cy + dy)
            cv2.line(color_debug, (x1, y1), (x2, y2), (0,255,0), 1)

    # print_debug=True Ïùº ÎïåÎßå ASCII Ï∂úÎ†•
    if print_debug:
        debug_resized = cv2.resize(color_debug, (80, 40))
        debug_gray = cv2.cvtColor(debug_resized, cv2.COLOR_BGR2GRAY)
        ascii_chars = ['.', '#']
        ascii_img = '\n'.join(
            ''.join(ascii_chars[1] if px>0 else ascii_chars[0] for px in row)
            for row in debug_gray
        )
        print(ascii_img)

    # üîπ ÏµúÏ¢Ö (64, 5 * max_edges * 4) Flatten ÌõÑ Î∞òÌôò (0~1 Ï†ïÍ∑úÌôî)
    final_tensor = torch.from_numpy(line_features_all).float().view(64, -1)
    max_val = torch.max(final_tensor)
    if max_val > 0:
        final_tensor /= max_val

    final_tensor = final_tensor.to(images.device)



    return final_tensor


def joint_vel_debug(env: ManagerBasedEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot"), print_debug: bool = True):
    """The joint velocities of the asset. If print_debug=True, prints them in terminal."""
    asset: Articulation = env.scene[asset_cfg.name]
    
    left_vel = asset.data.joint_vel[0, 0]
    right_vel = asset.data.joint_vel[0, 4]
    left_err = abs(env.action_manager.action[0,0] - left_vel)
    right_err = abs(env.action_manager.action[0,1] - right_vel)

    if print_debug:
        print(f"left_joint_vel = {left_vel:.1f} , right_joint_vel = {right_vel:.1f} , "
              f"{'ÏôºÏ™Ω' if left_vel < right_vel else 'Ïò§Î•∏Ï™Ω'} "
              f"{abs(left_vel - right_vel):.1f}  (Velocity Difference), "
              f"Minus is go ahead, vel unit: rad/s, torque unit: Nm")
        print(f"left_error = {left_err:.1f} , right_error = {right_err:.1f}")
        print(f"torque: {asset.data.applied_torque[0, [0,1,4,5]]}")
        print(f"acc= {asset.data.joint_acc[0, [0,1,4,5]]}")
        print(f"root_vel= {asset.data.root_lin_vel_w[0, 0]}")

    return asset.data.joint_vel[:, [0,1,4,5]]


def last_action_debug(env: ManagerBasedEnv, action_name: str | None = None, print_debug: bool = True) -> torch.Tensor:
    """
    The last input action to the environment.
    If print_debug=True, prints them in terminal.
    """
    if action_name is None:
        if print_debug:
            # os.system('clear' if os.name == 'posix' else 'cls')
            left_input = env.action_manager.action[0,0]
            right_input = env.action_manager.action[0,1]
            diff = abs(left_input - right_input)
            side = 'ÏôºÏ™Ω' if left_input < right_input else 'Ïò§Î•∏Ï™Ω'
            print(f"left_vel_input = {left_input:.1f} , right_vel_input = {right_input:.1f} rad/s, "
                  f"{side} = {diff:.1f} rad/s (Velocity Difference), "
                  f"Minus is go ahead, vel unit: rad/s, torque unit: Nm")

        return env.action_manager.action
    else:
        return env.action_manager.get_term(action_name).raw_actions
    
import torch
import numpy as np
import cv2
import os

def image_line_detection(
    env: ManagerBasedEnv,
    sensor_cfg: SceneEntityCfg = SceneEntityCfg("tiled_camera"),
    data_type: str = "rgb",
    convert_perspective_to_orthogonal: bool = False,
    normalize: bool = True,
    max_edges: int = 100,  # üîπ ÏµúÎåÄ Í≤ÄÏ∂úÌï† ÏÑ† Í∞úÏàò
    print_debug: bool = True  # üîπ ASCII ÎîîÎ≤ÑÍπÖ ÌôúÏÑ±Ìôî ÏòµÏÖò
) -> torch.Tensor:
    sensor: TiledCamera | Camera | RayCasterCamera = env.scene.sensors[sensor_cfg.name]
    images = sensor.data.output[data_type]  

    if (data_type == "distance_to_camera") and convert_perspective_to_orthogonal:
        images = math_utils.orthogonalize_perspective_depth(images, sensor.data.intrinsic_matrices)

    if normalize and data_type == "rgb":
        images = images.float() / 255.0
        mean_tensor = torch.mean(images, dim=(1, 2), keepdim=True)
        images -= mean_tensor
    elif "distance_to" in data_type or "depth" in data_type:
        images[images == float("inf")] = 0

    if data_type == "rgb":
        img_np = images.cpu().numpy()

        # üîπ RGB Îç∞Ïù¥ÌÑ∞ Î≤îÏúÑ Ï°∞Ï†ï (0~255Î°ú Î≥ÄÌôò)
        img_np = ((img_np + 1) * 127.5).astype(np.uint8)

        # üîπ RGB ‚Üí Grayscale Î≥ÄÌôò
        img_gray = np.zeros((img_np.shape[0], img_np.shape[1], img_np.shape[2]), dtype=np.uint8)
        for i in range(img_np.shape[0]):
            img_gray[i] = cv2.cvtColor(img_np[i], cv2.COLOR_RGB2GRAY)

        # üîπ CLAHE Ï†ÅÏö©
        enhanced_images = np.zeros_like(img_gray)
        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))
        for i in range(img_gray.shape[0]):
            enhanced_images[i] = clahe.apply(img_gray[i])

        # üîπ Canny Edge Detection
        edge_images = np.zeros_like(enhanced_images)
        for i in range(enhanced_images.shape[0]):
            edge_images[i] = cv2.Canny(enhanced_images[i], 5, 35)

        # üîπ HoughLinesP ÏÑ§Ï†ï
        rho = 1
        theta = np.pi / 30
        threshold = 10
        min_line_length = 30
        max_line_gap = 10

        batch_size = edge_images.shape[0]
        line_features_all = np.zeros((batch_size, max_edges * 4), dtype=np.float32)  # (batch, 30*4)

        for i in range(batch_size):
            lines = cv2.HoughLinesP(
                edge_images[i], rho, theta, threshold,
                minLineLength=min_line_length, maxLineGap=max_line_gap
            )

            frame_lines = []
            if lines is not None:
                lines = lines.reshape(-1, 4)  # (N, 4)

                # üîπ Ï§ëÏã¨ x Ï¢åÌëúÎ•º Í∏∞Ï§ÄÏúºÎ°ú Ï†ïÎ†¨
                sorted_lines = sorted(lines, key=lambda line: (line[0] + line[2]) / 2)

                # üîπ ÏµúÎåÄ max_edges Í∞úÎßå ÏÑ†ÌÉù
                frame_lines = sorted_lines[:max_edges]

            # üîπ Î∂ÄÏ°±Ìïú Í≤ΩÏö∞ (0,0,0,0) Ï±ÑÏö∞Í∏∞
            if len(frame_lines) < max_edges:
                frame_lines += [[0, 0, 0, 0]] * (max_edges - len(frame_lines))

            line_features_all[i] = np.array(frame_lines, dtype=np.float32).flatten()

        # üîπ Tensor Î≥ÄÌôò (batch, 30*4) ‚Üí (batch, -1) ÌòïÌÉúÎ°ú Î∞òÌôò
        line_tensor = torch.from_numpy(line_features_all).float().to(images.device)

        # üîπ ASCII ÎîîÎ≤ÑÍπÖ Ï∂îÍ∞Ä (print_debug=TrueÏùº ÎïåÎßå)
        if print_debug:
            os.system('clear' if os.name == 'posix' else 'cls')
            print("\nüñ•Ô∏è RealTime Edge Debug (ASCII) - First Frame\n" + "="*40)

            debug_img = np.zeros_like(edge_images[0])  # Í≤ÄÏ∂úÎêú ÏÑ†ÏùÑ ÏúÑÌïú Îπà Ïù¥ÎØ∏ÏßÄ

            # Ï≤´ Î≤àÏß∏ Î∞∞ÏπòÏùò Í≤ÄÏ∂úÎêú ÏÑ†ÏùÑ Í∑∏Î¶º
            lines_0 = line_features_all[0].reshape(max_edges, 4)
            for x1, y1, x2, y2 in lines_0:
                if (x1, y1, x2, y2) != (0, 0, 0, 0):  # Ìå®Îî©Îêú ÏÑ† Ï†úÏô∏
                    cv2.line(debug_img, (int(x1), int(y1)), (int(x2), int(y2)), 255, 1)

            # ASCII Î≥ÄÌôò
            debug_resized = cv2.resize(debug_img, (80, 40))  # Ï∂úÎ†•Ïö© ÌÅ¨Í∏∞ Ï°∞Ï†ï
            ascii_chars = ['.', '#']
            ascii_img = '\n'.join(
                ''.join(ascii_chars[1] if px > 0 else ascii_chars[0] for px in row)
                for row in debug_resized
            )
            print(ascii_img)

        return line_tensor  # (batch, -1) ÌòïÌÉúÏùò ÏÑ†Î∂Ñ Îç∞Ïù¥ÌÑ∞ Î∞òÌôò
import torch
import numpy as np
import cv2
import os

def image_gray(
    env: ManagerBasedEnv,
    sensor_cfg: SceneEntityCfg = SceneEntityCfg("tiled_camera"),
    data_type: str = "rgb",
    convert_perspective_to_orthogonal: bool = False,
    normalize: bool = True,
    max_edges: int = 30,  # üîπ ÏµúÎåÄ Í≤ÄÏ∂úÌï† ÏÑ† Í∞úÏàò
    print_debug: bool = True  # üîπ ASCII ÎîîÎ≤ÑÍπÖ ÌôúÏÑ±Ìôî ÏòµÏÖò
) -> torch.Tensor:
    sensor: TiledCamera | Camera | RayCasterCamera = env.scene.sensors[sensor_cfg.name]
    images = sensor.data.output[data_type]  

    if (data_type == "distance_to_camera") and convert_perspective_to_orthogonal:
        images = math_utils.orthogonalize_perspective_depth(images, sensor.data.intrinsic_matrices)

    if normalize and data_type == "rgb":
        images = images.float() / 255.0
        mean_tensor = torch.mean(images, dim=(1, 2), keepdim=True)
        images -= mean_tensor
    elif "distance_to" in data_type or "depth" in data_type:
        images[images == float("inf")] = 0

    if data_type == "rgb":
        img_np = images.cpu().numpy()

        # üîπ RGB Îç∞Ïù¥ÌÑ∞ Î≤îÏúÑ Ï°∞Ï†ï (0~255Î°ú Î≥ÄÌôò)
        img_np = ((img_np + 1) * 127.5).astype(np.uint8)

        # üîπ RGB ‚Üí Grayscale Î≥ÄÌôò
        img_gray = np.zeros((img_np.shape[0], img_np.shape[1], img_np.shape[2]), dtype=np.uint8)
        for i in range(img_np.shape[0]):
            img_gray[i] = cv2.cvtColor(img_np[i], cv2.COLOR_RGB2GRAY)

        # üîπ CLAHE Ï†ÅÏö©
        enhanced_images = np.zeros_like(img_gray)
        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))
        for i in range(img_gray.shape[0]):
            enhanced_images[i] = clahe.apply(img_gray[i])

        # üîπ Canny Edge Detection
        edge_images = np.zeros_like(enhanced_images)
        for i in range(enhanced_images.shape[0]):
            edge_images[i] = cv2.Canny(enhanced_images[i], 5, 30)

        # üîπ HoughLinesP ÏÑ§Ï†ï
        rho = 1
        theta = np.pi / 30
        threshold = 10
        min_line_length = 30
        max_line_gap = 10

        # üîπ ÏÑ†Î∂Ñ Í≤ÄÏ∂ú Î∞è Í∑∏Î¶¨Í∏∞
        line_images = np.zeros_like(edge_images)  # (batch, height, width)
        
        for i in range(edge_images.shape[0]):
            lines = cv2.HoughLinesP(
                edge_images[i], rho, theta, threshold,
                minLineLength=min_line_length, maxLineGap=max_line_gap
            )

            if lines is not None:
                lines = lines.reshape(-1, 4)  # (N, 4)

                # üîπ Ï§ëÏã¨ x Ï¢åÌëúÎ•º Í∏∞Ï§ÄÏúºÎ°ú Ï†ïÎ†¨
                sorted_lines = sorted(lines, key=lambda line: (line[0] + line[2]) / 2)

                # üîπ ÏµúÎåÄ max_edges Í∞úÎßå ÏÑ†ÌÉù
                sorted_lines = sorted_lines[:max_edges]

                # üîπ Í≤ÄÏ∂úÎêú ÏÑ†ÏùÑ Ïù¥ÎØ∏ÏßÄÏóê Í∑∏Î¶¨Í∏∞
                for x1, y1, x2, y2 in sorted_lines:
                    cv2.line(line_images[i], (x1, y1), (x2, y2), 255, 1)

        # üîπ **Ï±ÑÎÑêÏùÑ 3Í∞úÎ°ú ÌôïÏû• (Í∏∞Ï°¥ ÌòïÏãù Ïú†ÏßÄ)**
        line_images = np.repeat(line_images[:, :, :, np.newaxis], 3, axis=-1)  # (batch, height, width, 3)

        # üîπ Tensor Î≥ÄÌôò (Í∏∞Ï°¥ ÏΩîÎìú Ïú†ÏßÄ)
        contour_tensor = torch.from_numpy(line_images).float() / 255.0
        contour_tensor = contour_tensor.to(images.device)

        # üîπ ASCII ÎîîÎ≤ÑÍπÖ Ï∂îÍ∞Ä (print_debug=TrueÏùº ÎïåÎßå)
        if print_debug:
            os.system('clear' if os.name == 'posix' else 'cls')
            print("\nüñ•Ô∏è RealTime Edge Debug (ASCII) - First Frame\n" + "="*40)

            debug_img = line_images[0, :, :, 0]  # Ï≤´ Î≤àÏß∏ ÌôòÍ≤ΩÏùò ÏÑ†Î∂Ñ Ïù¥ÎØ∏ÏßÄ (Îã®Ïùº Ï±ÑÎÑê)
            debug_resized = cv2.resize(debug_img, (80, 40))  # ASCII Ï∂úÎ†•Ïö© ÌÅ¨Í∏∞ Ï°∞Ï†ï
            ascii_chars = ['.', '#']

            ascii_img = '\n'.join(
                ''.join(ascii_chars[1] if px > 0 else ascii_chars[0] for px in row)
                for row in debug_resized
            )
            print(ascii_img)

        return contour_tensor  # (batch, height, width, 3) ÌòïÌÉúÏùò ÏÑ†Î∂Ñ Îç∞Ïù¥ÌÑ∞ Î∞òÌôò


class image_features_gray(ManagerTermBase):
    """Extracted image features from a pre-trained frozen encoder.

    This method calls the :meth:`image` function to retrieve images, and then performs
    inference on those images.
    """

    def __init__(self, cfg: ObservationTermCfg, env: ManagerBasedEnv):
        super().__init__(cfg, env)
        from torchvision import models
        from transformers import AutoModel

        def create_theia_model(model_name):
            return {
                "model": (
                    lambda: AutoModel.from_pretrained(f"theaiinstitute/{model_name}", trust_remote_code=True)
                    .eval()
                    .to("cuda:0")
                ),
                "preprocess": lambda img: (img - torch.amin(img, dim=(1, 2), keepdim=True)) / (
                    torch.amax(img, dim=(1, 2), keepdim=True) - torch.amin(img, dim=(1, 2), keepdim=True)
                ),
                "inference": lambda model, images: model.forward_feature(
                    images, do_rescale=False, interpolate_pos_encoding=True
                ),
            }

        def create_resnet_model(resnet_name):
            return {
                "model": lambda: getattr(models, resnet_name)(pretrained=True).eval().to("cuda:0"),
                "preprocess": lambda img: (
                    img.permute(0, 3, 1, 2)  # Convert [batch, height, width, 3] -> [batch, 3, height, width]
                    - torch.tensor([0.485, 0.456, 0.406], device=img.device).view(1, 3, 1, 1)
                ) / torch.tensor([0.229, 0.224, 0.225], device=img.device).view(1, 3, 1, 1),
                "inference": lambda model, images: model(images),
            }

        # List of Theia models
        theia_models = [
            "theia-tiny-patch16-224-cddsv",
            "theia-tiny-patch16-224-cdiv",
            "theia-small-patch16-224-cdiv",
            "theia-base-patch16-224-cdiv",
            "theia-small-patch16-224-cddsv",
            "theia-base-patch16-224-cddsv",
        ]

        # List of ResNet models
        resnet_models = ["resnet18", "resnet34", "resnet50", "resnet101"]

        self.default_model_zoo_cfg = {}

        # Add Theia models to the zoo
        for model_name in theia_models:
            self.default_model_zoo_cfg[model_name] = create_theia_model(model_name)

        # Add ResNet models to the zoo
        for resnet_name in resnet_models:
            self.default_model_zoo_cfg[resnet_name] = create_resnet_model(resnet_name)

        self.model_zoo_cfg = self.default_model_zoo_cfg
        self.model_zoo = {}

    def __call__(
        self,
        env: ManagerBasedEnv,
        sensor_cfg: SceneEntityCfg = SceneEntityCfg("tiled_camera"),
        data_type: str = "rgb",
        convert_perspective_to_orthogonal: bool = False,
        model_zoo_cfg: dict | None = None,
        model_name: str = "ResNet18",
        model_device: str | None = "cuda:0",
        reset_model: bool = False,
    ) -> torch.Tensor:
        """Extracted image features from a pre-trained frozen encoder.

        Args:
            env: The environment.
            sensor_cfg: The sensor configuration to poll. Defaults to SceneEntityCfg("tiled_camera").
            data_type: THe sensor configuration datatype. Defaults to "rgb".
            convert_perspective_to_orthogonal: Whether to orthogonalize perspective depth images.
                This is used only when the data type is "distance_to_camera". Defaults to False.
            model_zoo_cfg: Map from model name to model configuration dictionary. Each model
                configuration dictionary should include the following entries:
                - "model": A callable that returns the model when invoked without arguments.
                - "preprocess": A callable that processes the images and returns the preprocessed results.
                - "inference": A callable that, when given the model and preprocessed images,
                    returns the extracted features.
            model_name: The name of the model to use for inference. Defaults to "ResNet18".
            model_device: The device to store and infer models on. This can be used help offload
                computation from the main environment GPU. Defaults to "cuda:0".
            reset_model: Initialize the model even if it already exists. Defaults to False.

        Returns:
            torch.Tensor: the image features, on the same device as the image
        """
        if model_zoo_cfg is not None:  # use other than default
            self.model_zoo_cfg.update(model_zoo_cfg)

        if model_name not in self.model_zoo or reset_model:
            # The following allows to only load a desired subset of a model zoo into GPU memory
            # as it becomes needed, in a "lazy" evaluation.
            print(f"[INFO]: Adding {model_name} to the model zoo")
            self.model_zoo[model_name] = self.model_zoo_cfg[model_name]["model"]()
        if model_device is not None:
            first_param = next(self.model_zoo[model_name].parameters(), None)
            if first_param is not None and first_param.device != torch.device(model_device):
                self.model_zoo[model_name] = self.model_zoo[model_name].to(model_device)


        images = image_gray(
            env=env,
            sensor_cfg=sensor_cfg,
            data_type=data_type,
            convert_perspective_to_orthogonal=convert_perspective_to_orthogonal,
            normalize=True,
            print_debug=True
            # want this for training stability
        )

        image_device = images.device

        if model_device is not None:
            images = images.to(model_device)

        proc_images = self.model_zoo_cfg[model_name]["preprocess"](images)
        features = self.model_zoo_cfg[model_name]["inference"](self.model_zoo[model_name], proc_images)

        return features.to(image_device).clone()
